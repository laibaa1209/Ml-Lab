{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extensive EDA — Data Cleaning Only (No Graphs)\n",
        "This notebook performs deep exploratory data analysis focusing strictly on **data cleaning**, including:\n",
        "- Missing value detection & handling\n",
        "- Duplicate checking\n",
        "- Outlier identification (no graphs)\n",
        "- Data type correction\n",
        "- String/category normalization\n",
        "- Feature consistency checks\n",
        "- Basic descriptive stats (non-visual)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset here\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Basic Information & Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()\n",
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Missing Value Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_values = df.isnull().sum().sort_values(ascending=False)\n",
        "percent_missing = (df.isnull().mean()*100).sort_values(ascending=False)\n",
        "pd.concat([missing_values, percent_missing], axis=1, keys=['Missing Count', '% Missing'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example missing value handling\n",
        "df_cleaned = df.copy()\n",
        "\n",
        "# Numeric columns - fill with median\n",
        "                \n",
        "num_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
        "df_cleaned[num_cols] = df_cleaned[num_cols].fillna(df_cleaned[num_cols].median())\n",
        "\n",
        "# Categorical columns - fill with mode\n",
        "cat_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
        "for col in cat_cols:\n",
        "    df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Duplicate Detection & Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicates = df_cleaned.duplicated().sum()\n",
        "print('Total Duplicates:', duplicates)\n",
        "\n",
        "df_cleaned = df_cleaned.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Outlier Detection (Non-Graphical) — IQR Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_outliers_iqr(series):\n",
        "    Q1 = series.quantile(0.25)\n",
        "    Q3 = series.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    return ((series < lower) | (series > upper)).sum()\n",
        "\n",
        "outliers = {col: detect_outliers_iqr(df_cleaned[col]) for col in num_cols}\n",
        "outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Type Corrections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example conversion\n",
        "for col in df_cleaned.columns:\n",
        "    if 'date' in col.lower():\n",
        "        df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='coerce')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Category/Label Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in cat_cols:\n",
        "    df_cleaned[col] = df_cleaned[col].str.strip().str.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Final Cleaned Data Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d583ab6",
      "metadata": {},
      "source": [
        "# 8. Replacing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e35554c",
      "metadata": {},
      "outputs": [],
      "source": [
        "df['age'] = df['age'].mask(df['age'] < 0, df['age'].median())\n",
        "df.replace({'gender': {'malee':'male'}}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c87387",
      "metadata": {},
      "source": [
        "# 9. Class imbalances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bf3a484",
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "X = df_cleaned.drop('target', axis=1)\n",
        "y = df_cleaned['target']\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f91f1416",
      "metadata": {},
      "source": [
        "# 10. Encodings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9939c513",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# certain catefories\n",
        "'''\n",
        "Converts each category into a unique integer.\n",
        "Good for ordinal variables or binary categories.\n",
        "'''\n",
        "le = LabelEncoder()\n",
        "df['color_label'] = le.fit_transform(df['color'])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0818bec",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Creates dummy variables for each category.\n",
        "Avoids introducing order when none exists.\n",
        "'''\n",
        "df_onehot = pd.get_dummies(df, columns=['color'], drop_first=True)\n",
        "print(df_onehot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5701fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ordinal Encoding, when order matters:\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "df = pd.DataFrame({'size': ['small', 'medium', 'large', 'medium', 'small']})\n",
        "\n",
        "encoder = OrdinalEncoder(categories=[['small', 'medium', 'large']])\n",
        "df['size_encoded'] = encoder.fit_transform(df[['size']])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dfd655d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# frequency encoding\n",
        "df = pd.DataFrame({'city': ['NY', 'LA', 'NY', 'SF', 'LA', 'NY']})\n",
        "freq = df['city'].value_counts()/len(df)\n",
        "df['city_freq'] = df['city'].map(freq)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "967e4be1",
      "metadata": {},
      "source": [
        "# 11. Standard Scaler:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b54947d",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cols = df.select_dtypes(include='number').columns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = df.copy()\n",
        "df_scaled[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "df_scaled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff505407",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = df.copy()\n",
        "df_scaled[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "df_scaled.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
